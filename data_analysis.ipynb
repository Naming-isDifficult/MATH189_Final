{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "Versions:\n",
    "- raw (under `./data/raw`): Original data extracted from html\n",
    "- preprocessed (under `./data/preprocessed`): Preprocessed data that mainly aim to remove missing values and/or parsing some strings.\n",
    "- finalized (under `./data/finalized`): Actual data we are going to use, features are shown below\n",
    "\n",
    "Features:\n",
    "- Instructor: Name of the instructor\n",
    "- Course: Name of the course,\n",
    "- Term: In which term the course is held\n",
    "- Enroll: Number of students enrolled\n",
    "- Evals Made: Number of evaluations made (does not affect Avg Grade Expected)\n",
    "- Rcmnd Class: Proportion of students who recommend class\n",
    "- Rcmnd Instr: Proportion of students who recommend instructor\n",
    "- Study Hrs/wk: Study hours per week\n",
    "- Avg Grade Expected: Expected average grade (among evaluations made)\n",
    "- Avg Grade Received: Actual average grade (among all students)\n",
    "- isPreGPT: GPT hasn't been introduced when the course in live\n",
    "- isSTEM: The course is a STEM course\n",
    "- isAbstract: The course is either proof based or focusing on abstract concepts that GPT is not good at (valid if `isSTEM=True`)\n",
    "- isWritten: The course is mainly based on written materials instead of audio/video (valid if `isSTEM=False`)\n",
    "- isUD: The course is upper division course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scripts.data_analysis import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "\n",
    "np.random.seed(189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "path = './data/finalized'\n",
    "csv_files = []\n",
    "\n",
    "for file in glob.glob(os.path.join(path, '*')):\n",
    "    if os.path.isfile(file):\n",
    "        csv_files.append(file)\n",
    "\n",
    "df = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "- **Descriptive Statistics**\n",
    "  - Summary statistics for key features.\n",
    "- **Visualizations**\n",
    "  - Histograms, box plots, and other relevant visualizations to understand the distribution of grades and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Dividing data into several sub-groups\n",
    "\n",
    "All of them should undergoes pretty similar structure:\n",
    "1. Compare distributions (i.e. histogram, boxplot, etc.)\n",
    "2. Statistical tests\n",
    "3. Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEM Lower Division Courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEM_lowerDivision_preGPT vs. STEM_lowerDivision_postGPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out data\n",
    "params = {\n",
    "    'is_ud': False,\n",
    "    'is_stem': True,\n",
    "    'is_abstract': False,\n",
    "    'is_written': False # no effect\n",
    "}\n",
    "\n",
    "pre_gpt, post_gpt = filter_data(df, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_dist(pre_gpt, post_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping t-test\n",
    "observed_t, bootstrap_t, p_vals = bootstrap_ttest(pre_gpt['Avg Grade Received'], post_gpt['Avg Grade Received'])\n",
    "\n",
    "plot_bootstrap_test(observed_t, bootstrap_t, p_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arts Lower Division Courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arts_lowerDivision_writtenBased_preGPT vs. Arts_lowerDivision_writtenBased_postGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out data\n",
    "params = {\n",
    "    'is_ud': False,\n",
    "    'is_stem': False,\n",
    "    'is_abstract': False, # no effect\n",
    "    'is_written': True\n",
    "}\n",
    "\n",
    "pre_gpt, post_gpt = filter_data(df, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_dist(pre_gpt, post_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping t-test\n",
    "observed_t, bootstrap_t, p_vals = bootstrap_ttest(pre_gpt['Avg Grade Received'], post_gpt['Avg Grade Received'])\n",
    "\n",
    "plot_bootstrap_test(observed_t, bootstrap_t, p_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arts_lowerDivision_notWrittenBased_preGPT vs. Arts_lowerDivision_notWrittenBased_postGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out data\n",
    "params = {\n",
    "    'is_ud': False,\n",
    "    'is_stem': False,\n",
    "    'is_abstract': False, # no effect\n",
    "    'is_written': False\n",
    "}\n",
    "\n",
    "pre_gpt, post_gpt = filter_data(df, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_dist(pre_gpt, post_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping t-test\n",
    "observed_t, bootstrap_t, p_vals = bootstrap_ttest(pre_gpt['Avg Grade Received'], post_gpt['Avg Grade Received'])\n",
    "\n",
    "plot_bootstrap_test(observed_t, bootstrap_t, p_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEM Upper Division Courses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEM_upperDivision_nonAbstract_preGPT vs. STEM_upperDivision_nonAbstract_postGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out data\n",
    "params = {\n",
    "    'is_ud': True,\n",
    "    'is_stem': True,\n",
    "    'is_abstract': False, \n",
    "    'is_written': False # no effect\n",
    "}\n",
    "\n",
    "pre_gpt, post_gpt = filter_data(df, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_dist(pre_gpt, post_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping t-test\n",
    "observed_t, bootstrap_t, p_vals = bootstrap_ttest(pre_gpt['Avg Grade Received'], post_gpt['Avg Grade Received'])\n",
    "\n",
    "plot_bootstrap_test(observed_t, bootstrap_t, p_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEM_upperDivision_abstract_preGPT vs. STEM_upperDivision_abstract_postGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out data\n",
    "params = {\n",
    "    'is_ud': True,\n",
    "    'is_stem': True,\n",
    "    'is_abstract': True, \n",
    "    'is_written': False # no effect\n",
    "}\n",
    "\n",
    "pre_gpt, post_gpt = filter_data(df, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_dist(pre_gpt, post_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping t-test\n",
    "observed_t, bootstrap_t, p_vals = bootstrap_ttest(pre_gpt['Avg Grade Received'], post_gpt['Avg Grade Received'])\n",
    "\n",
    "plot_bootstrap_test(observed_t, bootstrap_t, p_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arts Upper Division Courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arts_upperDivision_writtenBased_preGPT vs. Arts_upperDivision_writtenBased_postGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'is_ud': True,\n",
    "    'is_stem': False,\n",
    "    'is_abstract': False, # no effect \n",
    "    'is_written': True\n",
    "}\n",
    "\n",
    "pre_gpt, post_gpt = filter_data(df, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_dist(pre_gpt, post_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping t-test\n",
    "observed_t, bootstrap_t, p_vals = bootstrap_ttest(pre_gpt['Avg Grade Received'], post_gpt['Avg Grade Received'])\n",
    "\n",
    "plot_bootstrap_test(observed_t, bootstrap_t, p_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arts_upperDivision_notWrittenBased_preGPT vs. Arts_upperDivision_notWrittenBased_postGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'is_ud': True,\n",
    "    'is_stem': False,\n",
    "    'is_abstract': False, # no effect \n",
    "    'is_written': False\n",
    "}\n",
    "\n",
    "pre_gpt, post_gpt = filter_data(df, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plot_dist(pre_gpt, post_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping t-test\n",
    "observed_t, bootstrap_t, p_vals = bootstrap_ttest(pre_gpt['Avg Grade Received'], post_gpt['Avg Grade Received'])\n",
    "\n",
    "plot_bootstrap_test(observed_t, bootstrap_t, p_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential cross comparisons\n",
    "Not quite sure what to include at this time but there should be some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
